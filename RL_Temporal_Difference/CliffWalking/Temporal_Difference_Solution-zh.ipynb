{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迷你项目：时间差分方法\n",
    "\n",
    "在此 notebook 中，你将自己编写很多时间差分 (TD) 方法的实现。\n",
    "\n",
    "虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。\n",
    "\n",
    "### 第 0 部分：探索 CliffWalkingEnv\n",
    "\n",
    "请使用以下代码单元格创建 [CliffWalking](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py) 环境的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "智能体会在 $4\\times 12$ 网格世界中移动，状态编号如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    " [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    " [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    " [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在任何阶段开始时，初始状态都是状态 `36`。状态 `47`是唯一的终止状态，悬崖对应的是状态 `37` 到 `46`。\n",
    "\n",
    "智能体可以执行 4 个潜在动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，$\\mathcal{S}^+=\\{0, 1, \\ldots, 47\\}$ 以及 $\\mathcal{A} =\\{0, 1, 2, 3\\}$。请通过运行以下代码单元格验证这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discrete(4)\n",
    "Discrete(48)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此迷你项目中，我们将逐步发现 CliffWalking 环境的最优策略。最优状态值函数可视化结果如下。请立即花时间确保理解_为何_ 这是最优状态值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plot_utils import plot_values\n",
    "\n",
    "# define the optimal state-value function\n",
    "V_opt = np.zeros((4,12))\n",
    "V_opt[0:13][0] = -np.arange(3, 15)[::-1]\n",
    "V_opt[0:13][1] = -np.arange(3, 15)[::-1] + 1\n",
    "V_opt[0:13][2] = -np.arange(3, 15)[::-1] + 2\n",
    "V_opt[3][0] = -13\n",
    "\n",
    "plot_values(V_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<matplotlib.figure.Figure at 0x7f4415081588>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 1 部分：TD 预测 - 状态值\n",
    "\n",
    "\n",
    "在此部分，你将自己编写 TD 预测的实现（用于估算状态值函数）。\n",
    "\n",
    "我们首先将研究智能体按以下方式移动的策略：\n",
    "- 在状态 `0` 到 `10`（含）时向 `RIGHT` 移动， \n",
    "- 在状态 `11`、`23` 和 `35` 时向 `DOWN` 移动，\n",
    "- 在状态 `12` 到 `22`（含）、状态 `24` 到 `34`（含）和状态 `36` 时向 `UP`移动。\n",
    "\n",
    "下面指定并输出了该策略。注意，智能体没有选择动作的状态被标记为 `-1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.hstack([1*np.ones(11), 2, 0, np.zeros(10), 2, 0, np.zeros(10), 2, 0, -1*np.ones(11)])\n",
    "print(\"\\nPolicy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy.reshape(4,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\n",
    "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.]\n",
    " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.]\n",
    " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.]\n",
    " [ 0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请运行下个单元格，可视化与此策略相对应的状态值函数。你需要确保花时间来理解为何这是对应的值函数！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_true = np.zeros((4,12))\n",
    "for i in range(3):\n",
    "    V_true[0:12][i] = -np.arange(3, 15)[::-1] - i\n",
    "V_true[1][11] = -2\n",
    "V_true[2][11] = -1\n",
    "V_true[3][0] = -17\n",
    "\n",
    "plot_values(V_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_10_0.png)\n",
    "\n",
    "\n",
    "你将通过 TD 预测算法尝试逼近上图的结果。\n",
    "\n",
    "你的 TD 预测算法将包括 5 个参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `policy`：这是一个一维 numpy 数组，其中 `policy.shape` 等于状态数量 (`env.nS`)。`policy[s]` 返回智能体在状态 `s` 时选择的动作。\n",
    "- `alpha`：这是更新步骤的步长参数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "\n",
    "- `V`：这是一个字典，其中 `V[s]` 是状态 `s` 的估算值。\n",
    "\n",
    "请完成以下代码单元格中的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import sys\n",
    "\n",
    "def td_prediction(env, num_episodes, policy, alpha, gamma=1.0):\n",
    "    # initialize empty dictionaries of floats\n",
    "    V = defaultdict(float)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # choose action A\n",
    "            action = policy[state]\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # perform updates\n",
    "            V[state] = V[state] + (alpha * (reward + (gamma * V[next_state]) - V[state]))            \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # end episode if reached terminal state\n",
    "            if done:\n",
    "                break   \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请运行以下代码单元格，以测试你的实现并可视化估算的状态值函数。如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！你可以随意更改提供给该函数的 `num_episodes` 和 `alpha` 参数。但是，如果你要确保单元测试的准确性，请勿更改 `gamma` 的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_test\n",
    "\n",
    "# evaluate the policy and reshape the state-value function\n",
    "V_pred = td_prediction(env, 5000, policy, .01)\n",
    "\n",
    "# please do not change the code below this line\n",
    "V_pred_plot = np.reshape([V_pred[key] if key in V_pred else 0 for key in np.arange(48)], (4,12)) \n",
    "check_test.run_check('td_prediction_check', V_pred_plot)\n",
    "plot_values(V_pred_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 5000/5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**\n",
    "\n",
    "\n",
    "\n",
    "![png](output_14_2.png)\n",
    "\n",
    "\n",
    "你的估算状态值函数与该策略对应的真状态值函数有多接近？  \n",
    "\n",
    "你可能注意到了，有些状态值不是智能体估算的。因为根据该策略，智能体不会经历所有状态。在 TD 预测算法中，智能体只能估算所经历的状态对应的值。\n",
    "\n",
    "### 第 2 部分：TD 控制 - Sarsa\n",
    "\n",
    "在此部分，你将自己编写 Sarsa 控制算法的实现。\n",
    "\n",
    "你的算法将有四个参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `alpha`：这是更新步骤的步长参数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "\n",
    "- `Q`：这是一个字典（一维数组），其中 `Q[s][a]` 是状态 `s` 和动作 `a` 对应的估算动作值。\n",
    "\n",
    "请完成以下代码单元格中的函数。\n",
    "\n",
    "（_你可以随意定义其他函数，以帮助你整理代码。_）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Qsa, Qsa_next, reward, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent time step \"\"\"\n",
    "    return Qsa + (alpha * (reward + (gamma * Qsa_next) - Qsa))\n",
    "\n",
    "def epsilon_greedy_probs(env, Q_s, i_episode, eps=None):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    epsilon = 1.0 / i_episode\n",
    "    if eps is not None:\n",
    "        epsilon = eps\n",
    "    policy_s = np.ones(env.nA) * epsilon / env.nA\n",
    "    policy_s[np.argmax(Q_s)] = 1 - epsilon + (epsilon / env.nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()   \n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()   \n",
    "        # get epsilon-greedy action probabilities\n",
    "        policy_s = epsilon_greedy_probs(env, Q[state], i_episode)\n",
    "        # pick action A\n",
    "        action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "        # limit number of time steps per episode\n",
    "        for t_step in np.arange(300):\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            if not done:\n",
    "                # get epsilon-greedy action probabilities\n",
    "                policy_s = epsilon_greedy_probs(env, Q[next_state], i_episode)\n",
    "                # pick next action A'\n",
    "                next_action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "                # update TD estimate of Q\n",
    "                Q[state][action] = update_Q(Q[state][action], Q[next_state][next_action], \n",
    "                                            reward, alpha, gamma)\n",
    "                # S <- S'\n",
    "                state = next_state\n",
    "                # A <- A'\n",
    "                action = next_action\n",
    "            if done:\n",
    "                # update TD estimate of Q\n",
    "                Q[state][action] = update_Q(Q[state][action], 0, reward, alpha, gamma)\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请使用下个代码单元格可视化**_估算的_**最优策略和相应的状态值函数。  \n",
    "\n",
    "如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！你可以随意更改提供给该函数的 `num_episodes` 和 `alpha` 参数。但是，如果你要确保单元测试的准确性，请勿更改 `gamma` 的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_sarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "plot_values(V_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 5000/5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_20_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Average Reward over 100 Episodes:  -13.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\n",
    "[[ 3  1  0  1  1  1  1  2  0  1  1  2]\n",
    " [ 2  1  2  2  1  0  1  0  1  3  2  2]\n",
    " [ 1  1  1  1  1  1  1  1  1  1  1  2]\n",
    " [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_20_5.png)\n",
    "\n",
    "\n",
    "### 第 3 部分：TD 控制 - Q 学习\n",
    "\n",
    "在此部分，你将自己编写 Q 学习控制算法的实现。\n",
    "\n",
    "你的算法将有四个参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `alpha`：这是更新步骤的步长参数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "- `Q`：这是一个字典（一维数组），其中 `Q[s][a]` 是状态 `s` 和动作 `a` 对应的估算动作值。\n",
    "\n",
    "请完成以下代码单元格中的函数。\n",
    "\n",
    "（_你可以随意定义其他函数，以帮助你整理代码。_）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # get epsilon-greedy action probabilities\n",
    "            policy_s = epsilon_greedy_probs(env, Q[state], i_episode)\n",
    "            # pick next action A\n",
    "            action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            # update Q\n",
    "            Q[state][action] = update_Q(Q[state][action], np.max(Q[next_state]), \\\n",
    "                                                  reward, alpha, gamma)        \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # until S is terminal\n",
    "            if done:\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请使用下个代码单元格可视化**_估算的_**最优策略和相应的状态值函数。 \n",
    "\n",
    "如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！你可以随意更改提供给该函数的 `num_episodes` 和 `alpha` 参数。但是，如果你要确保单元测试的准确性，请勿更改 `gamma` 的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsamax = q_learning(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsamax = np.array([np.argmax(Q_sarsamax[key]) if key in Q_sarsamax else -1 for key in np.arange(48)]).reshape((4,12))\n",
    "check_test.run_check('td_control_check', policy_sarsamax)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsamax)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_sarsamax[key]) if key in Q_sarsamax else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 5000/5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_24_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Average Reward over 100 Episodes:  -13.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\n",
    "[[ 1  0  2  1  1  1  0  1  0  1  1  1]\n",
    " [ 0  1  1  1  3  1  1  1  0  2  2  2]\n",
    " [ 1  1  1  1  1  1  1  1  1  1  1  2]\n",
    " [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_24_5.png)\n",
    "\n",
    "\n",
    "### 第 4 部分：TD 控制 - 预期 Sarsa\n",
    "\n",
    "在此部分，你将自己编写预期 Sarsa 控制算法的实现。\n",
    "\n",
    "你的算法将有四个参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `alpha`：这是更新步骤的步长参数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "- `Q`：这是一个字典（一维数组），其中 `Q[s][a]` 是状态 `s` 和动作 `a` 对应的估算动作值。\n",
    "\n",
    "请完成以下代码单元格中的函数。\n",
    "\n",
    "（_你可以随意定义其他函数，以帮助你整理代码。_）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode\n",
    "        state = env.reset()\n",
    "        # get epsilon-greedy action probabilities\n",
    "        policy_s = epsilon_greedy_probs(env, Q[state], i_episode, 0.005)\n",
    "        while True:\n",
    "            # pick next action\n",
    "            action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            # get epsilon-greedy action probabilities (for S')\n",
    "            policy_s = epsilon_greedy_probs(env, Q[next_state], i_episode, 0.005)\n",
    "            # update Q\n",
    "            Q[state][action] = update_Q(Q[state][action], np.dot(Q[next_state], policy_s), \\\n",
    "                                                  reward, alpha, gamma)        \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # until S is terminal\n",
    "            if done:\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请使用下个代码单元格可视化**_估算的_**最优策略和相应的状态值函数。  \n",
    "\n",
    "如果代码单元格返回 **PASSED**，则表明你正确地实现了该函数！你可以随意更改提供给该函数的 `num_episodes` 和 `alpha` 参数。但是，如果你要确保单元测试的准确性，请勿更改 `gamma` 的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_expsarsa = expected_sarsa(env, 10000, 1)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_expsarsa = np.array([np.argmax(Q_expsarsa[key]) if key in Q_expsarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_expsarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_expsarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_expsarsa[key]) if key in Q_expsarsa else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 10000/10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_28_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Average Reward over 100 Episodes:  -13.03\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color: green;\">PASSED</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\n",
    "[[ 1  1  1  3  1  1  1  2  1  1  2  2]\n",
    " [ 1  1  1  1  1  1  1  1  1  1  1  2]\n",
    " [ 1  1  1  1  1  1  1  1  1  1  1  2]\n",
    " [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_28_5.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
